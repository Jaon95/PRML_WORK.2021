{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture + Naive Bayes\n",
    "We implement the Gaussian mixture naive Bayes model to predict Santander Customer Transaction Prediction data. The problem has a binary target and 200 continuous features, and we assume that these features are conditionally independent given the class. We model the target  Y  as Bernoulli, taking values 0 (negative) and 1 (positive). The features  $$X_0,X_1,…,X_{199}$$ are modelled as continuous random variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\frac{p_Y(y)\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y)}{\\sum_{y'=0}^1p_Y(y')\\prod_{i=0}^{199}f_{X_i|Y}(x_i|y')}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       202\n",
       "unique        1\n",
       "top       False\n",
       "freq        202\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##检查是否有缺失值\n",
    "train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>...</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>...</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>...</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>...</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              target          var_0          var_1          var_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.100490      10.679914      -1.627622      10.715192   \n",
       "std         0.300653       3.040051       4.050044       2.640894   \n",
       "min         0.000000       0.408400     -15.043400       2.117100   \n",
       "25%         0.000000       8.453850      -4.740025       8.722475   \n",
       "50%         0.000000      10.524750      -1.608050      10.580000   \n",
       "75%         0.000000      12.758200       1.358625      12.516700   \n",
       "max         1.000000      20.315000      10.376800      19.353000   \n",
       "\n",
       "               var_3          var_4          var_5          var_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.796529      11.078333      -5.065317       5.408949   \n",
       "std         2.043319       1.623150       7.863267       0.866607   \n",
       "min        -0.040200       5.074800     -32.562600       2.347300   \n",
       "25%         5.254075       9.883175     -11.200350       4.767700   \n",
       "50%         6.825000      11.108250      -4.833150       5.385100   \n",
       "75%         8.324100      12.261125       0.924800       6.003000   \n",
       "max        13.188300      16.671400      17.251600       8.447700   \n",
       "\n",
       "               var_7          var_8  ...        var_190        var_191  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean       16.545850       0.284162  ...       3.234440       7.438408   \n",
       "std         3.418076       3.332634  ...       4.559922       3.023272   \n",
       "min         5.349700     -10.505500  ...     -14.093300      -2.691700   \n",
       "25%        13.943800      -2.317800  ...      -0.058825       5.157400   \n",
       "50%        16.456800       0.393700  ...       3.203600       7.347750   \n",
       "75%        19.102900       2.937900  ...       6.406200       9.512525   \n",
       "max        27.691800      10.151300  ...      18.440900      16.716500   \n",
       "\n",
       "             var_192        var_193        var_194        var_195  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.927839       3.331774      17.993784      -0.142088   \n",
       "std         1.478423       3.992030       3.135162       1.429372   \n",
       "min        -3.814500     -11.783400       8.694400      -5.261000   \n",
       "25%         0.889775       0.584600      15.629800      -1.170700   \n",
       "50%         1.901300       3.396350      17.957950      -0.172700   \n",
       "75%         2.949500       6.205800      20.396525       0.829600   \n",
       "max         8.402400      18.281800      27.928800       4.272900   \n",
       "\n",
       "             var_196        var_197        var_198        var_199  \n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000  \n",
       "mean        2.303335       8.908158      15.870720      -3.326537  \n",
       "std         5.454369       0.921625       3.010945      10.438015  \n",
       "min       -14.209600       5.960600       6.299300     -38.852800  \n",
       "25%        -1.946925       8.252800      13.829700     -11.208475  \n",
       "50%         2.408900       8.888200      15.934050      -2.819550  \n",
       "75%         6.556725       9.593300      18.064725       4.836800  \n",
       "max        18.321500      12.000400      26.079100      28.500700  \n",
       "\n",
       "[8 rows x 201 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe() #查看数据的均值、方差信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:, 2:].values.astype('float64')\n",
    "y_train = train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python机器学习\\其他\\.venv\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(y_train, palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20098"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train[y_train==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_time(values):\n",
    "    t = time.strftime('%H:%M:%S',time.gmtime(values))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "adjusted_mutual_info_score\n",
      "adjusted_rand_score\n",
      "average_precision\n",
      "balanced_accuracy\n",
      "completeness_score\n",
      "explained_variance\n",
      "f1\n",
      "f1_macro\n",
      "f1_micro\n",
      "f1_samples\n",
      "f1_weighted\n",
      "fowlkes_mallows_score\n",
      "homogeneity_score\n",
      "jaccard\n",
      "jaccard_macro\n",
      "jaccard_micro\n",
      "jaccard_samples\n",
      "jaccard_weighted\n",
      "max_error\n",
      "mutual_info_score\n",
      "neg_brier_score\n",
      "neg_log_loss\n",
      "neg_mean_absolute_error\n",
      "neg_mean_absolute_percentage_error\n",
      "neg_mean_gamma_deviance\n",
      "neg_mean_poisson_deviance\n",
      "neg_mean_squared_error\n",
      "neg_mean_squared_log_error\n",
      "neg_median_absolute_error\n",
      "neg_root_mean_squared_error\n",
      "normalized_mutual_info_score\n",
      "precision\n",
      "precision_macro\n",
      "precision_micro\n",
      "precision_samples\n",
      "precision_weighted\n",
      "r2\n",
      "rand_score\n",
      "recall\n",
      "recall_macro\n",
      "recall_micro\n",
      "recall_samples\n",
      "recall_weighted\n",
      "roc_auc\n",
      "roc_auc_ovo\n",
      "roc_auc_ovo_weighted\n",
      "roc_auc_ovr\n",
      "roc_auc_ovr_weighted\n",
      "top_k_accuracy\n",
      "v_measure_score\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "for i in sorted(metrics.SCORERS.keys()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape of train: (140000, 200)\n",
      "data shape of test (60000, 200)\n",
      "fitting finished!\n",
      "time consumed of fitting: 00:01:38\n",
      "scoring finished!\n",
      "time consumed of scoring: 00:00:00\n",
      "test acc of logistics regression without PCA :  0.9139333333333334\n"
     ]
    }
   ],
   "source": [
    "#下面使用线性回归来进行训练\n",
    "def logit_regresstion():\n",
    "    lg_x_train, lg_x_test, lg_y_train, lg_y_test = model_selection.train_test_split(X_train,y_train,test_size=0.3,random_state=200)\n",
    "    print('data shape of train:',lg_x_train.shape)\n",
    "    print('data shape of test',lg_x_test.shape)\n",
    "    lr1 = LogisticRegression(C=9,dual=False,max_iter=lg_x_train.shape[0])\n",
    "    start_time = time.time()\n",
    "    lr1.fit(lg_x_train , lg_y_train)\n",
    "    print('fitting finished!')\n",
    "    end_time = time.time()\n",
    "    print('time consumed of fitting:',show_time(end_time-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lg_score = lr1.score(lg_x_test, lg_y_test)\n",
    "    end_time = time.time()\n",
    "    print('scoring finished!')\n",
    "    print('time consumed of scoring:',show_time(end_time-start_time))\n",
    "    print('test acc of logistics regression without PCA : ',lg_score)\n",
    "logit_regresstion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维后的数据训练数据 shape： (140000, 178)\n",
      "降维后的测试数据shape:  (60000, 178)\n",
      "fitting finished!\n",
      "time consumed of fitting: 00:00:00\n",
      "scoring finished!\n",
      "time consumed of scoring: 00:00:00\n",
      "test acc of logistics regression with PCA :  0.9139333333333334\n"
     ]
    }
   ],
   "source": [
    "def logit_regresstion_with_pca():\n",
    "    lg_x_train, lg_x_test, lg_y_train, lg_y_test = model_selection.train_test_split(X_train,y_train,test_size=0.3,random_state=200)\n",
    "\n",
    "    #标准化\n",
    "    std_scal = StandardScaler().fit(lg_x_train)\n",
    "    lg_x_train_std = std_scal.transform(lg_x_train)\n",
    "\n",
    "\n",
    "    #主成分分析，通过求协方差矩阵的特征向量，利用特征向量构成的矩阵对原来的x进行转化-构成顺序按特征值大小逆序排序，转化后的特征数量是和x的特征数量相等的\n",
    "    sklearn_pca = sklearnPCA().fit(lg_x_train_std)\n",
    "\n",
    "    #查看主成分分析后的个成分的方差信息，并且归一化\n",
    "    var_per = sklearn_pca.explained_variance_ratio_\n",
    "    cum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "    l = len(cum_var_per[cum_var_per <= 0.9])\n",
    "    sklearn_pca = sklearnPCA(n_components=l).fit(lg_x_train_std)\n",
    "\n",
    "    # 使用主成分分析的结果，对原特征进行较为\n",
    "    lg_x_train = sklearn_pca.transform(lg_x_train_std)\n",
    "    lg_x_test = sklearn_pca.transform(std_scal.transform(lg_x_test))\n",
    "    print('降维后的数据训练数据 shape：',lg_x_train.shape)\n",
    "    print('降维后的测试数据shape: ',lg_x_test.shape)\n",
    "\n",
    "    lr1 = LogisticRegression(C=9,dual=False,max_iter=lg_x_train.shape[0])\n",
    "    start_time = time.time()\n",
    "    lr1.fit(lg_x_train , lg_y_train)\n",
    "    print('fitting finished!')\n",
    "    end_time = time.time()\n",
    "    print('time consumed of fitting:',show_time(end_time-start_time))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lg_score = lr1.score(lg_x_test, lg_y_test)\n",
    "    end_time = time.time()\n",
    "    print('scoring finished!')\n",
    "    print('time consumed of scoring:',show_time(end_time-start_time))\n",
    "    print('test acc of logistics regression with PCA : ',lg_score)\n",
    "logit_regresstion_with_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Acquainted with the Gaussian Mixture Model\n",
    "The Gaussian mixture model gives a mixture of normal distrubutions. We can use **sklearn.mixture.GaussianMixture** to fit the data and compare it with the histogram to get a feel of its behaviour. We also need to standardize the features because too narrow data can impair the fitting ability of the Gaussian mixture model. There are two important hyperparameters: n_components is the number of normal distributions to mix in and reg_covar is a regularization parameter that controls the spread of the bumps. Note that **score_samples** method gives the log density, so we need to exponentiate to get the density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Model\n",
    "We are going to use the Gaussian mixture model to estimate the likelihood probability density functions $f_{X_i|Y}(x_i|y)$.Since multiplying a lot of small numbers will lead to underflow, we take the logarithm and turn products into sums.\n",
    "$$\\ln p_{Y|X_0,X_1,\\ldots,X_{199}}(y|x_0,x_1,\\ldots,x_{199})=\\underbrace{\\overbrace{\\ln p_Y(y)}^\\text{log prior}+\\overbrace{\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y)}^\\text{log likelihood}}_\\text{log joint}-\\overbrace{\\ln\\sum_{y'=0}^1e^{\\ln p_Y(y')+\\sum_{i=0}^{199}\\ln f_{X_i|Y}(x_i|y')}}^\\text{log marginal}$$\n",
    "Key points in the implementation are:\n",
    "+ The log prior is set as the logarithm of the proportion of different classes.\n",
    "+ The log likelihood is computed by using **sklearn.mixture.GaussianMixture's score_samples** method.\n",
    "+ Computing the log marginal is prone to overflow/underflow, so we use **scipy.special.logsumexp** to avoid that.\n",
    "+ In the end, we convert back to probability by exponentiation.\n",
    "\n",
    "All the heavy lifting is done by the Gaussian mixture model. The rest of the computation is very simple and fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9, 5, 2, 6, 4, 9, 2, 2, 2, 0, 3, 9, 4, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "a = [np.random.randint(0,10) for i in range(16)]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =np.array(a).reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[[True,True,False,False],0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "class GaussianMixtureNB(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_components=1, reg_covar=1e-06):\n",
    "        self.n_components = n_components #有几个高斯函数\n",
    "        self.reg_covar = reg_covar #正则化\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.log_prior_ = np.log(np.bincount(y) / len(y)) #计算每个类型的y在y中出现的频率，然后取对数\n",
    "        # shape of self.log_pdf_\n",
    "        shape = (len(self.log_prior_), X.shape[1]) #前面是y的种类，后面是指标\n",
    "        self.log_pdf_ = [[GaussianMixture(n_components=self.n_components,\n",
    "                                          reg_covar=self.reg_covar) #混合高斯分布模型\n",
    "                          .fit(X[y == i, j:j + 1]) #这个是在使用em算法对模型的参数进行训练,score_, 前一个参数表示样本值，后一个表示特征数\n",
    "                          .score_samples for j in range(shape[1])] #针对每个类型的每种中标，建立一个高斯模型\n",
    "                         for i in range(shape[0])] #i表示种类\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # shape of log_likelihood before summing\n",
    "        shape = (len(self.log_prior_), X.shape[1], X.shape[0])#第一个表示种类，第二个表示指标，第三个表示样本量\n",
    "        log_likelihood = np.sum([[self.log_pdf_[i][j](X[:, j:j + 1])\n",
    "                                  for j in range(shape[1])]\n",
    "                                 for i in range(shape[0])], axis=1).T##似然计算\n",
    "        log_joint = self.log_prior_ + log_likelihood\n",
    "        return np.exp(log_joint - logsumexp(log_joint, axis=1, keepdims=True))# 得到每种类型的概率\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).argmax(axis=1)#返回概率最大的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model\n",
    "We train and evaluate the model by using the training AUC and validation AUC. The process can take time because we train the Gaussian mixture model 400 times, and the training time will increase with higher n_components. In order to speed up the hyperparameter search, we use validation, which is k times faster than k-fold cross-validation. Feel free to use cross-validation if you have the time (and tell me if you find better hyperparameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000 20000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "i_train, i_valid = next(StratifiedShuffleSplit(n_splits=1).split(X_train, y_train))\n",
    "print(len(i_train),len(i_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC is 0.9077447370600885.\n",
      "Validation AUC is 0.8989165069593666.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pipeline = make_pipeline(StandardScaler(),\n",
    "                         GaussianMixtureNB(n_components=10, reg_covar=0.03))\n",
    "pipeline.fit(X_train[i_train], y_train[i_train])\n",
    "pipeline.score(X_train[i_valid],y_train[i_valid]) # Evaluate the accuracy\n",
    "print('Training AUC is {}.'\n",
    "      .format(roc_auc_score(y_train[i_train],\n",
    "                            pipeline.predict_proba(X_train[i_train])[:, 1])))\n",
    "print('Validation AUC is {}.'\n",
    "      .format(roc_auc_score(y_train[i_valid],\n",
    "                            pipeline.predict_proba(X_train[i_valid])[:, 1])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "99ae8d1c855da79370c0f19852a01459aa89d91f414e4084eab559d3968b050b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
